{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import config\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import copy, deepcopy\n",
    "import cmath\n",
    "import chess\n",
    "from utils import *\n",
    "\n",
    "from agents import *\n",
    "from environments import *\n",
    "from models import *\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision = 3)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2987212\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "agent = Agent(board_logic = BoardLogic(max_num_moves=100), \n",
    "              in_ch=20, \n",
    "              ch=128, \n",
    "              n_blocks=10,\n",
    "              sample_policy=eps_greedy_policy,)\n",
    "\n",
    "print(sum(p.numel() for p in agent.online_net1.parameters() if p.requires_grad))\n",
    "environment = Environment(max_num_moves=100,\n",
    "                          filter_blunder=False, # causes a lot of draw in early self-play of on\n",
    "                          )\n",
    "\n",
    "opt_list = [None, None]\n",
    "\n",
    "model = Model(agent = agent,\n",
    "               environment = environment,\n",
    "               mem_capacity = 1000000,\n",
    "               init_mem = False,\n",
    "               batch_size = 512,\n",
    "               num_warmup = 100000,\n",
    "               policy_update = 2,\n",
    "               tau = 0.01,\n",
    "               temp_scaler = TemperatureScaler(temp_start=0.5, \n",
    "                                               temp_end=0.25, \n",
    "                                               temp_min=5e-2, \n",
    "                                               episode_decay=5000, \n",
    "                                               transition_decay=0.9),\n",
    "               opt_list=opt_list,\n",
    "               scaler=torch.amp.GradScaler(\"cuda\")\n",
    "             )\n",
    "\n",
    "optimizer_grouped_parameters1 = group_decay_parameters(\n",
    "    agent.online_net1,\n",
    "    weight_decay=1e-5,\n",
    "    no_decay=[\"bias\", \"GroupNorm.weight\"],\n",
    "    )\n",
    "\n",
    "optimizer_grouped_parameters2 = group_decay_parameters(\n",
    "    agent.online_net2,\n",
    "    weight_decay=1e-5,\n",
    "    no_decay=[\"bias\", \"GroupNorm.weight\"],\n",
    "    )\n",
    "\n",
    "opt_list[0] = torch.optim.AdamW(optimizer_grouped_parameters1, lr=1e-4)\n",
    "opt_list[1] = torch.optim.AdamW(optimizer_grouped_parameters2, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=100, \n",
    "                                                       filter_blunder=False), \n",
    "                             num_games=500,\n",
    "                             temp = 0.2)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_core(model, filename=\"model_5000_episodes_core.pth\")\n",
    "save_memory(model, filename=\"model_5000_episodes_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=100, \n",
    "                                                       filter_blunder=False), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_core(model, filename=\"model_eps_10000_episodes_core.pth\")\n",
    "save_memory(model, filename=\"model_eps_10000_episodes_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=100, \n",
    "                                                       filter_blunder=False), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_core(model, filename=\"model_eps_15000_episodes_core.pth\")\n",
    "save_memory(model, filename=\"model_eps_15000_episodes_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=100, \n",
    "                                                       filter_blunder=False), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_core(model, filename=\"model_eps_20000_episodes_core.pth\")\n",
    "save_memory(model, filename=\"model_eps_20000_episodes_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With boltzmann\n",
    "\n",
    "model = load_checkpoint(\"model_eps_20000_episodes_core.pth\", \"model_eps_20000_episodes_memory.pth\", model)\n",
    "\n",
    "model.agent.sample_policy = eps_softmax_policy\n",
    "\n",
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=100, \n",
    "                                                       filter_blunder=False), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_core(model, filename=\"model_eps_25000_episodes_core.pth\")\n",
    "save_memory(model, filename=\"model_eps_25000_episodes_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue with eps\n",
    "\n",
    "model = load_checkpoint(\"model_eps_20000_episodes_core.pth\", \"model_eps_20000_episodes_memory.pth\", model)\n",
    "\n",
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=100, \n",
    "                                                       filter_blunder=False), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_core(model, filename=\"model_eps_25000_episodes_core.pth\")\n",
    "save_memory(model, filename=\"model_eps_25000_episodes_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint(\"model_eps_25000_episodes_core.pth\", \"model_eps_25000_episodes_memory.pth\", model)\n",
    "\n",
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=100, \n",
    "                                                       filter_blunder=False), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_core(model, filename=\"model_eps_30000_episodes_core.pth\")\n",
    "save_memory(model, filename=\"model_eps_30000_episodes_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=100, \n",
    "                                                       filter_blunder=False), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_core(model, filename=\"model_eps_35000_episodes_core.pth\")\n",
    "save_memory(model, filename=\"model_eps_35000_episodes_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=100, \n",
    "                                                       filter_blunder=False), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_core(model, filename=\"model_eps_40000_episodes_core.pth\")\n",
    "save_memory(model, filename=\"model_eps_40000_episodes_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123f576481bf47f3808b38e0e9839251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5925ddce123349d0977a1e2913d3ac6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 176, -1: 167, 0: 157} tensor(0.0007, device='cuda:0') 0.06926783991142844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc24fa64ec248deb32bbda0cca19476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 195, -1: 151, 0: 154} tensor(0.0007, device='cuda:0') 0.06784952987691713\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30283ab805704eda8ec2204ccbf11473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 202, -1: 151, 0: 147} tensor(0.0005, device='cuda:0') 0.06423700579040337\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a573952441e746368473390e65684b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 203, -1: 147, 0: 150} tensor(0.0004, device='cuda:0') 0.07187870628355691\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3024fa649391463fbbee3798f41210f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 188, -1: 173, 0: 139} tensor(0.0005, device='cuda:0') 0.069119074476118\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint(\"model_eps_40000_episodes_core.pth\", \"model_eps_40000_episodes_memory.pth\", model)\n",
    "\n",
    "model.temp_scaler.temp_min = 0.0001\n",
    "\n",
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=100, \n",
    "                                                       filter_blunder=False), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_core(model, filename=\"model_eps_45000_episodes_core.pth\")\n",
    "save_memory(model, filename=\"model_eps_45000_episodes_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f1ade398e64dbc81bb3e402891c7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=100, \n",
    "                                                       filter_blunder=False), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_core(model, filename=\"model_eps_50000_episodes_core.pth\")\n",
    "save_memory(model, filename=\"model_eps_50000_episodes_memory.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint(\"model_eps_40000_episodes_core.pth\", None, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment(max_num_moves=100, filter_blunder=False)\n",
    "environment.reset()\n",
    "#random.seed(42)\n",
    "#np.random.seed(42)\n",
    "#torch.manual_seed(42)\n",
    "temp = 0.1\n",
    "environment.board = chess.Board('rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNB1KBNR w KQkq - 0 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = agent.select_action(environment, temp=temp, greedy=True)\n",
    "move = agent.action_to_move(action)\n",
    "\n",
    "board, (reward, done) = environment.step(move)\n",
    "\n",
    "state = agent.board_logic.board_to_state(board).to(config.device)\n",
    "\n",
    "Q1 = agent.online_net1(state).detach()\n",
    "Q2 = agent.online_net2(state).detach()\n",
    "legal_moves = environment.get_legal_moves()\n",
    "mask_legal = agent.get_mask_legal(legal_moves)\n",
    "\n",
    "Q1_legal = Q1[mask_legal]\n",
    "Q2_legal = Q2[mask_legal]\n",
    "\n",
    "diff = torch.abs(Q1_legal - Q2_legal)\n",
    "\n",
    "print(f\"{np.mean(diff.cpu().numpy()):.4f}\")\n",
    "\n",
    "Q_legal = Q1.masked_fill(~mask_legal, -1e9)\n",
    "action_star = torch.argmax(Q_legal, dim=1).to(config.device)\n",
    "score = Q2[0,action_star[0]]\n",
    "\n",
    "if environment.mirror:\n",
    "    print(\"Black:\")\n",
    "    print(f\"score: {score.item():.4f}\")\n",
    "    print(board.mirror())\n",
    "else:\n",
    "    print(\"White:\")\n",
    "    print(f\"score: {score.item():.4f}\")\n",
    "    print(board)\n",
    "\n",
    "\n",
    "if board.is_checkmate():\n",
    "    print(\"checkmate!\")\n",
    "\n",
    "Q_legal = Q1.masked_fill(~mask_legal, -1e5)\n",
    "#print(Q_legal[Q_legal>-1])\n",
    "\n",
    "q_max = Q_legal.max(dim=1, keepdim=True).values\n",
    "print(Q_legal[Q_legal>-100])\n",
    "#print(q_max)\n",
    "logits = (Q_legal - q_max)\n",
    "#print(torch.sort(logits, descending=True).values[0,:10])\n",
    "probs = torch.softmax(logits/0.1, dim=1)\n",
    "print(probs[logits>-2])\n",
    "temp = temp*0.95\n",
    "print(f\"temp: {temp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[0,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.board_to_state(environment.get_board())[0,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint(\"model_conv_55000_episodes_core.pth\", \"model_conv_55000_episodes_memory.pth\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = chess.Board()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
