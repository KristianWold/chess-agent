{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA toolkit version PyTorch was built with: 12.8\n",
      "cuDNN version: 90701\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../src/')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import config\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import copy, deepcopy\n",
    "import cmath\n",
    "import chess\n",
    "from utils import saver, loader\n",
    "\n",
    "from agents import *\n",
    "from environments import *\n",
    "from models import *\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision = 3)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)  \n",
    "print(\"CUDA toolkit version PyTorch was built with:\", torch.version.cuda)  \n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version()) \n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2980300\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "agent = Agent(board_logic = BoardLogic(), in_ch=14, ch=128, n_blocks=10)\n",
    "print(sum(p.numel() for p in agent.online_net1.parameters() if p.requires_grad))\n",
    "environment = Environment(max_num_moves=200,)\n",
    "\n",
    "opt_list = [None, None]\n",
    "\n",
    "model = Model(agent = agent,\n",
    "               environment = environment,\n",
    "               mem_capacity = 1000000,\n",
    "               batch_size = 512,\n",
    "               num_warmup = 100000,\n",
    "               policy_update = 2,\n",
    "               tau = 0.01,\n",
    "               temp_scaler = TemperatureScaler(temp_start=0.6, \n",
    "                                               temp_end=0.3, \n",
    "                                               temp_min=1e-5, \n",
    "                                               episode_decay=5000, \n",
    "                                               transition_decay=0.95),\n",
    "               opt_list=opt_list,\n",
    "               scaler=torch.amp.GradScaler(\"cuda\")\n",
    "             )\n",
    "\n",
    "optimizer_grouped_parameters1 = group_decay_parameters(\n",
    "    agent.online_net1,\n",
    "    weight_decay=1e-5,\n",
    "    no_decay=[\"bias\", \"GroupNorm.weight\"],\n",
    "    )\n",
    "\n",
    "optimizer_grouped_parameters2 = group_decay_parameters(\n",
    "    agent.online_net2,\n",
    "    weight_decay=1e-5,\n",
    "    no_decay=[\"bias\", \"GroupNorm.weight\"],\n",
    "    )\n",
    "\n",
    "opt_list[0] = torch.optim.AdamW(optimizer_grouped_parameters1, lr=1e-4)\n",
    "opt_list[1] = torch.optim.AdamW(optimizer_grouped_parameters2, lr=1e-4)\n",
    "\n",
    "\n",
    "eval_agents = EvaluateAgents(agent1 = agent, \n",
    "                             agent2 = deepcopy(agent), \n",
    "                             environment = Environment(max_num_moves=200,), \n",
    "                             num_games=200,\n",
    "                             temp = 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=200,), \n",
    "                             num_games=100,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_checkpoint(model, filename=\"model_conv_5000_episodes_large.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=200,), \n",
    "                             num_games=100,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_checkpoint(model, filename=\"model_conv_10000_episodes_large.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=200,), \n",
    "                             num_games=100,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_checkpoint(model, filename=\"model_conv_15000_episodes_large.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint(\"model_conv_15000_episodes_large.pth\", model)\n",
    "\n",
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=200,), \n",
    "                             num_games=100,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_checkpoint(model, filename=\"model_conv_20000_episodes_large.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint(\"model_conv_20000_episodes_large.pth\", model)\n",
    "\n",
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=200,), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_checkpoint(model, filename=\"model_conv_25000_episodes_large.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint(\"model_conv_25000_episodes_large.pth\", model)\n",
    "\n",
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=200,), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_checkpoint(model, filename=\"model_conv_30000_episodes_large.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint(\"model_conv_30000_episodes_large.pth\", model)\n",
    "\n",
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=200,), \n",
    "                             num_games=500,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_checkpoint(model, filename=\"model_conv_35000_episodes_large.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a631154fc7ce438da0776871bbcae633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6e17cb575f4de3aff03bcc40e465dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 38, -1: 34, 0: 128} tensor(0.0013, device='cuda:0') 0.11869323918796784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3fd19cf08d4c18a6d52343b1bcbf4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 33, -1: 24, 0: 143} tensor(0.0009, device='cuda:0') 0.11177589415627368\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      5\u001b[39m model.environment.filter_blunder = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      8\u001b[39m eval_agents = EvaluateAgents(agent1 = agent1, \n\u001b[32m      9\u001b[39m                              agent2 = agent2, \n\u001b[32m     10\u001b[39m                              environment = Environment(max_num_moves=\u001b[32m200\u001b[39m, \n\u001b[32m     11\u001b[39m                                                       filter_blunder=\u001b[38;5;28;01mTrue\u001b[39;00m), \n\u001b[32m     12\u001b[39m                              num_games=\u001b[32m200\u001b[39m,\n\u001b[32m     13\u001b[39m                              temp = \u001b[32m0.25\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m model.train(num_episodes = \u001b[32m5000\u001b[39m, \n\u001b[32m     16\u001b[39m             evaluate_agents = eval_agents,\n\u001b[32m     17\u001b[39m             freq=\u001b[32m1000\u001b[39m)\n\u001b[32m     19\u001b[39m save_checkpoint(model, filename=\u001b[33m\"\u001b[39m\u001b[33mmodel_conv_40000_episodes_large.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/chess-agent/notebooks/Experiments/../../src/models.py:144\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, num_episodes, evaluate_agents, freq)\u001b[39m\n\u001b[32m    141\u001b[39m state = state_next\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m counter % \u001b[38;5;28mself\u001b[39m.policy_update == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.optimize_agent()\n\u001b[32m    146\u001b[39m counter += \u001b[32m1\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28mself\u001b[39m.temp_scaler.step_transition()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/chess-agent/notebooks/Experiments/../../src/models.py:211\u001b[39m, in \u001b[36mModel.optimize_agent\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.compute_loss(online_net, \n\u001b[32m    202\u001b[39m                              target_net, \n\u001b[32m    203\u001b[39m                              state_batch, \n\u001b[32m   (...)\u001b[39m\u001b[32m    207\u001b[39m                              reward_batch, \n\u001b[32m    208\u001b[39m                              done_batch)\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Optimize the model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.scale(loss).backward()\n\u001b[32m    213\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.unscale_(opt)\n\u001b[32m    214\u001b[39m params = [p \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m opt.param_groups \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m g[\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/env_pytorch/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m torch.autograd.backward(\n\u001b[32m    649\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    650\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/env_pytorch/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m _engine_run_backward(\n\u001b[32m    354\u001b[39m     tensors,\n\u001b[32m    355\u001b[39m     grad_tensors_,\n\u001b[32m    356\u001b[39m     retain_graph,\n\u001b[32m    357\u001b[39m     create_graph,\n\u001b[32m    358\u001b[39m     inputs,\n\u001b[32m    359\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    360\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    361\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/env_pytorch/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    825\u001b[39m         t_outputs, *args, **kwargs\n\u001b[32m    826\u001b[39m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = load_checkpoint(\"model_conv_35000_episodes_large.pth\", model)\n",
    "\n",
    "agent1 = model.agent\n",
    "agent2 = deepcopy(agent1)\n",
    "model.environment.filter_blunder = True\n",
    "\n",
    "\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=200, \n",
    "                                                      filter_blunder=True), \n",
    "                             num_games=200,\n",
    "                             temp = 0.25)\n",
    "\n",
    "model.train(num_episodes = 5000, \n",
    "            evaluate_agents = eval_agents,\n",
    "            freq=1000)\n",
    "\n",
    "save_checkpoint(model, filename=\"model_conv_40000_episodes_large.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9667b33ef5df491081b2a713570a19e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 41, -1: 32, 0: 127}\n"
     ]
    }
   ],
   "source": [
    "print(eval_agents.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0f82d4bad34dbca9f4cf0c84b23d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 33, -1: 36, 0: 31}\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint(\"model_conv_35000_episodes_large.pth\", model)\n",
    "agent1 = deepcopy(model.agent)\n",
    "model = load_checkpoint(\"model_conv_35000_episodes_large.pth\", model)\n",
    "agent2 = model.agent\n",
    "\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=200), \n",
    "                             num_games=100,\n",
    "                             temp = 0.25)\n",
    "\n",
    "results = eval_agents.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143624f9e3044af5a3afbc4acd8d8d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 18, -1: 19, 0: 63}\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint(\"model_conv_35000_episodes_large.pth\", model)\n",
    "agent1 = deepcopy(model.agent)\n",
    "model = load_checkpoint(\"model_conv_35000_episodes_large.pth\", model)\n",
    "agent2 = model.agent\n",
    "\n",
    "eval_agents = EvaluateAgents(agent1 = agent1, \n",
    "                             agent2 = agent2, \n",
    "                             environment = Environment(max_num_moves=200,filter_blunder=True), \n",
    "                             num_games=100,\n",
    "                             temp = 0.25)\n",
    "\n",
    "results = eval_agents.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_checkpoint(\"model_conv_35000_episodes_large.pth\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment(max_num_moves=200,filter_blunder=True)\n",
    "environment.reset()\n",
    "#random.seed(42)\n",
    "#np.random.seed(42)\n",
    "#torch.manual_seed(42)\n",
    "temp = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2543\n",
      "Black:\n",
      "score: -0.2364\n",
      "r n b q . b . r\n",
      "p p p p . k . .\n",
      ". . . . . n . p\n",
      ". . . . P p . P\n",
      ". . . . . P . .\n",
      ". . . . P . . .\n",
      "P P P . . . . .\n",
      "R N B Q K B N R\n",
      "tensor([[-0.1655]], device='cuda:0')\n",
      "tensor([ 0.0000, -0.2062, -0.3657, -0.4584, -0.5254, -0.5477, -0.6247, -0.6523,\n",
      "        -0.6536, -0.6556], device='cuda:0')\n",
      "tensor([0.0881, 0.0557, 0.0457, 0.0336, 0.0456, 0.0459, 0.0427, 0.0435, 0.0458,\n",
      "        0.0509, 0.0361, 0.0717, 0.0472, 0.0521, 0.0611], device='cuda:0')\n",
      "temp: 0.0463\n"
     ]
    }
   ],
   "source": [
    "action = agent.select_action(environment, temp=temp, greedy=False)\n",
    "move = agent.action_to_move(action)\n",
    "\n",
    "board, (reward, done) = environment.step(move)\n",
    "\n",
    "state = agent.board_logic.board_to_state(board).to(config.device)\n",
    "\n",
    "Q1 = agent.online_net1(state).detach()\n",
    "Q2 = agent.online_net2(state).detach()\n",
    "legal_moves = environment.get_legal_moves()\n",
    "mask_legal = agent.get_mask_legal(legal_moves)\n",
    "\n",
    "Q1_legal = Q1[mask_legal]\n",
    "Q2_legal = Q2[mask_legal]\n",
    "\n",
    "diff = torch.abs(Q1_legal - Q2_legal)/torch.max(torch.abs(Q1_legal), torch.abs(Q2_legal))\n",
    "\n",
    "print(f\"{np.mean(diff.cpu().numpy()):.4f}\")\n",
    "\n",
    "Q_legal = Q1.masked_fill(~mask_legal, -1e9)\n",
    "action_star = torch.argmax(Q_legal, dim=1).to(config.device)\n",
    "score = Q2[0,action_star[0]]\n",
    "\n",
    "if environment.mirror:\n",
    "    print(\"Black:\")\n",
    "    print(f\"score: {score.item():.4f}\")\n",
    "    print(board.mirror())\n",
    "else:\n",
    "    print(\"White:\")\n",
    "    print(f\"score: {score.item():.4f}\")\n",
    "    print(board)\n",
    "\n",
    "\n",
    "if board.is_checkmate():\n",
    "    print(\"checkmate!\")\n",
    "\n",
    "Q_legal = Q1.masked_fill(~mask_legal, -float('inf'))\n",
    "#print(Q_legal[Q_legal>-1])\n",
    "\n",
    "q_max = Q_legal.max(dim=1, keepdim=True).values\n",
    "print(q_max)\n",
    "\n",
    "logits = (Q_legal - q_max)/temp\n",
    "print(torch.sort(logits, descending=True).values[0,:10])\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "print(probs[logits>-1])\n",
    "temp = temp*0.95\n",
    "print(f\"temp: {temp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.board.is_insufficient_material()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.move_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
